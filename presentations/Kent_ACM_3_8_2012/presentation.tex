\documentclass{beamer}
\usepackage{beamerthemesplit}
\usepackage{hyperref}

\begin{document}
\title{Near-Neighbor Search at Scale}
\author{Casey Stella}
\date{\today} 

\frame{\titlepage} 

\frame{\frametitle{Table of Contents}\tableofcontents} 

\section{Introduction}

\frame{\frametitle{Introduction}
\begin{itemize}
\item Hi, I'm Casey\pause
\item I work at Explorys\pause
\item I am a senior engineer on the high speed indexes team\pause
\item I am also a recovering Mathematician\pause
\item I use Hadoop and HBase (a NoSQL datastore) every day\pause
\item I'm here to talk about Near-Neighbor Searches\pause
\item In particular, about how they're hard to do efficiently at scale.
\end{itemize}
}

\section{Near Neighbor Search}
\frame{\frametitle{Near Neighbor Search}
\begin{itemize}
\item Given a point in a vector space, find the nearest points according to some metric\pause
   \begin{itemize}
   \item You probably know a few vector spaces, like $\mathbb{R}^2$ from Calculus\pause
   \item You probably know a metric or two like $L_2$ aka the Euclidean metric\pause
   \item Or $L_1$ a.k.a. Taxicab distance from A.I.\pause
   \end{itemize}
\item Many problems can be rephrased as a near neighbor search (or use it as a primary component)\pause
   \begin{itemize}
   \item Recommendation Systems
   \item Contextual Marketing (i.e. ads)
   \item Clustering data
   \item Lots more
   \end{itemize}
\end{itemize}
}
\frame{\frametitle{Traditional Approach}
\begin{itemize}
\item A na\"{\i}ve approach would be $O(n)$\pause
\item A less na\"{\i}ve approach typically involves $kd$-trees\pause
\item These tend to scale poorly in very high dimensions\pause
   \begin{itemize}
   \item The rule of thumb\footnote[1]{Jacob E. Goodman, Joseph O'Rourke and Piotr Indyk (Ed.) (2004). ``Chapter 39 : Nearest neighbours in high-dimensional spaces''. Handbook of Discrete and Computational Geometry (2nd ed.). CRC Press.} is, for dimension $k$ and number of points $N$, $N >> 2^{k}$\pause
   \item Otherwise you end up doing a nearly exhaustive search most of the time\pause
   \item In these situations, approximation algorithms are typically used\pause
   \end{itemize}
\item It's also not clear that they work for non-$L_2$ metrics
\end{itemize}
}

\section{Schemaless Data Stores}

\frame{\frametitle{``Big Data''}
\begin{itemize}
\item Sometimes we have to deal with large amounts of data\pause
\item Traditionally we've put that data in SQL tables\pause
\item Scaling SQL databases is a pain in the ass\pause
   \begin{itemize}
   \item Explicit sharding breaks joins\pause
   \item Have to worry about node availability yourself\pause
   \item A lot of engineering work\pause
   \end{itemize}
\item It's a hard problem and you have to give up many of the benefits of SQL to solve it.
\end{itemize}
}

\frame{\frametitle{Schema-less NoSQL data stores}
\begin{itemize}
\item Recently there has been a movement to use distributed schema-less data stores instead\pause
\item These also happen to be a pain in the ass\pause
   \begin{itemize}
   \item Don't hate the player, hate the scale\pause
   \end{itemize}
\item Conform to a map interface typically\pause
   \begin{itemize}
   \item put(Key k, Value v)
   \item get(Key k)
   \item delete(Key k)\pause
   \end{itemize}
\item Examples of these are HBase, Cassandra, MongoDB, MemcacheDB\pause
\item It would be very nice to be able to use this to find nearest neighbors, but how?
\end{itemize}
}

\section{Locality Sensitive Hashing}

\frame{\frametitle{Near Neighbor Searches}
\begin{itemize}
\item Often we {\bf need} high dimension (see previous talk)\pause
\item Often we have {\bf many} points\pause
\item Often we'll accept an approximation\pause
\item Often we are looking for data in a fixed radius.
\end{itemize}
}

\frame{\frametitle{Locality Sensitive Hashing}
\begin{itemize}
\item {\bf Locality Sensitive Hashing} is a probabalistic technique to group ``close'' vectors according to a given metric.\pause
\item We can use this to group our ``near'' vectors into buckets\pause
\item You can construct multiple hash functions (i.e. families) and compose to increase the accuracy at the expense of runtime complexity\pause
\item You can use multiple LSH functions and put the same input data into each bucket, thereby increasing accuracy at the expense of space complexity
\end{itemize}
}

\frame{\frametitle{Locality Sensitive Hashing: The Cons}
\begin{itemize}
\item Different hash function for different distance metrics (not all have them)\pause
   \begin{itemize}
   \item Exist for the biggest ones\pause
   \item $L_k$ for $k > 0$, cosine-distance, min-hash, kernel-based metrics (i.e. machine learned distance metrics)\pause
   \end{itemize}
\item Not all LSH functions have theoretical bounds about accuracy\pause
   \begin{itemize}
   \item Almost all research focuses on {\bf nearest} neighbor searches\pause
   \item Practical alternative is to sample your data and measure to determine parameters empirically
   \end{itemize}
\end{itemize}
}

\frame{\frametitle{Stable Distributions and the $L_k$ metric}
\begin{itemize}
\item Based on the research of Piotr Indyk, et. al.\footnote[2]{Datar, M.; Immorlica, N., Indyk, P., Mirrokni, V.S. (2004). ``Locality-Sensitive Hashing Scheme Based on p-Stable Distributions''. Proceedings of the Symposium on Computational Geometry.}\pause
\item They found that $1$-stable and $2$-stable distributions could be used to construct families of locality sensitive hashes for $L_1$ and $L_2$ metrics\pause
\item What the hell are $p$-stable distributions?\pause
   \begin{itemize}
   \item It's complicated and can be considered a black-box if you like (tune out for the next minute or so)\pause
   \item If you draw a vector $\mathbf{a}$ from a $p$-stable distribution $X$, $\mathbf{a}\cdot(\mathbf{v_1} - \mathbf{v_2})$ is distributed exactly as $||\mathbf{v_1} - \mathbf{v_2}||_pX$\pause
   \item Know that the Normal distribution is $2$-stable and the Cauchy distribution is $1$-stable
   \end{itemize}
\end{itemize}
}

\frame{\frametitle{Some Intuition}
\begin{itemize}
\item Take the real number line and split it up into segments of length $r$, we can assign each segment an index and hash vectors into these segments: $\mathbf{v} \rightarrow \lfloor\frac{\mathbf{a}\cdot\mathbf{v}}{r}\rfloor$.\pause
\item $\frac{\mathbf{a}\cdot(\mathbf{v_1} - \mathbf{v_2})}{r}$ is distributed as $\frac{||\mathbf{v_1} - \mathbf{v_2}||_p}{r}X$ due to $X$ being $p$-stable\pause
\item So $\mathbf{v_1}$ and $\mathbf{v_2}$ should map to the same segment with high probability if they're within radius $r$ of each other.\pause
\item Different choices of $\mathbf{a}$ can form families of these hash functions.
\end{itemize}
}

\section{Conclusion}

\frame{\frametitle{Spatial Search}
\begin{itemize}
\item I've begun creating a simple library to assist in the use of these locality sensitive hashes\pause
\item It's Datastore agnostic\pause
\item $L_1$ and $L_2$ are implemented as well as a utility to assist in choosing parameters\pause
\item Next up is min-hash\pause
\item \url{https://github.com/cestella/SpatialSearch}
\end{itemize}
}

\frame{\frametitle{Conclusion}
\begin{itemize}
\item Thanks for your attention
\item Follow me on twitter $@casey\_stella$
\item Find me at
  \begin{itemize}
  \item \url{http://caseystella.com}
  \item \url{https://github.com/cestella}
  \end{itemize}
\item PS. If you like this...Explorys is hiring!
\end{itemize}
}

\end{document}
